{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping and Machine Learning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides an example of how basic web scraping and in Python\n",
    "<ul>\n",
    "<li>IPython Notebooks</li>\n",
    "<li>Parsing HTML using [lxml](http://lxml.de/)</li>\n",
    "<li>Data manipulation w/ [pandas](http://pandas.pydata.org)</li>\n",
    "<li>Machine learning for classification: Multinomial Naive Bayes w/ [sklearn](http://scikit-learn.org)</li>\n",
    "</ul>\n",
    "\n",
    "Now lets see if we can teach our computer how to tell the difference between a Wall Street Journal headline and a Gawker headline. \n",
    "\n",
    "Inspiration for this notebook/presentation was provided by [this](http://nbviewer.ipython.org/github/nealcaren/workshop_2014/blob/master/notebooks/6_Classification.ipynb) and [this](http://nbviewer.ipython.org/github/cs109/content/blob/master/HW3_solutions.ipynb). If you want to run this notebook, or one like it, it'll be helpful for you to check out [Anaconda](https://store.continuum.io/cshop/anaconda/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data!\n",
    "If we want to build a model with a discerning taste for quality news, we're going to have to find him some examples. Lucky for us, Gawker and the Wall Street Journal both have websites. \n",
    "### To the internet (with dev tools)!\n",
    "#### [Gawker](http://gawker.com)\n",
    "After poking around the gawker homepage with the help of command + option + i (mac) and a Chrome plugin, [XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=en), we can see that all the article titles can be grabbed with the xpath, `\"//header/h1/a/text()\"`. lxml lets us retrieve all the text that matches our xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 22 titles. Here are the first 5:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['April 19 Has Become Everyone-is-a-Threat Day',\n",
       " 'BuzzFeed Deleted Posts Under Pressure from Its Own Business Department',\n",
       " \"Texas Vet Fired After Posting About Cat She'd Allegedly Killed\",\n",
       " 'Jon Stewart on ',\n",
       " ': \"I Think of Us as Turd Miners\"']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import html \n",
    "x = html.parse('http://gawker.com')\n",
    "titles = x.xpath('//header/h1/a/text()')\n",
    "print \"We got {} titles. Here are the first 5:\".format(len(titles))\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page only has 20 titles on it. If we want to properly train our model, we're going to need more examples. There's a \"More Stories\" button at the bottom of the page, which brings us to another, similarly structured page with new titles and another \"More Stories\" button. To get more examples, we'll repeat the process above in a loop with each successive iteration hitting the page pointed to by the \"More Stories\" button. In order to figure out what the link is, go do some more investigating! You can also just look at the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headlines = x.xpath('//header/h1/a/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 22 titles from url: http://gawker.com/\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1429306066246\n",
      "Retrieved 19 titles from url: http://gawker.com/?startTime=1429280486922\n",
      "Retrieved 24 titles from url: http://gawker.com/?startTime=1429209060638\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1429190460993\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1429127220147\n",
      "Retrieved 19 titles from url: http://gawker.com/?startTime=1429106400436\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1429048200300\n",
      "Retrieved 19 titles from url: http://gawker.com/?startTime=1429026600621\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428959400965\n",
      "Retrieved 24 titles from url: http://gawker.com/?startTime=1428936300788\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1428868800563\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1428765300817\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428686220452\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428619828246\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428598017878\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428533630242\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1428512700394\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428468300524\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428432902160\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1428415491015\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428349500880\n",
      "Retrieved 24 titles from url: http://gawker.com/?startTime=1428326639030\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428181200907\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428089040774\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1428071940282\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1428001500761\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1427982042940\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427915786268\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427867178334\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427827380942\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427806620837\n",
      "Retrieved 25 titles from url: http://gawker.com/?startTime=1427744057738\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427723880942\n",
      "Retrieved 25 titles from url: http://gawker.com/?startTime=1427564467579\n",
      "Retrieved 19 titles from url: http://gawker.com/?startTime=1427484928543\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427465941501\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1427396220792\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427337728157\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427307419671\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427243700797\n",
      "Retrieved 19 titles from url: http://gawker.com/?startTime=1427215800689\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1427146614006\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1427122548462\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1426972057864\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1426883400857\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1426860900316\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1426794991274\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1426775746041\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1426707000285\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1426650213998\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1426621800878\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1426606500831\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1426535450108\n",
      "Retrieved 24 titles from url: http://gawker.com/?startTime=1426513084708\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1426364700641\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1426274160188\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1426259656443\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1426193235683\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1426167720696\n",
      "Retrieved 19 titles from url: http://gawker.com/?startTime=1426097286699\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1426025100504\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1426001692522\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1425925077872\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1425842700718\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1425685063913\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1425661200202\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1425585120317\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1425516300388\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1425486300535\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1425415899245\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1425395944936\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1425324310557\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1425303300503\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1425157200465\n",
      "Retrieved 25 titles from url: http://gawker.com/?startTime=1425054600392\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1424970046045\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1424887020225\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1424804100899\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1424733018622\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1424668886528\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1424629800795\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1424474700095\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1424385900938\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1424300400435\n",
      "Retrieved 23 titles from url: http://gawker.com/?startTime=1424235900026\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1424194200481\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1424118300859\n",
      "Retrieved 24 titles from url: http://gawker.com/?startTime=1424039400455\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1423933500478\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1423842240693\n",
      "Retrieved 22 titles from url: http://gawker.com/?startTime=1423757400542\n",
      "Retrieved 21 titles from url: http://gawker.com/?startTime=1423668240915\n",
      "Retrieved 20 titles from url: http://gawker.com/?startTime=1423593244708\n"
     ]
    }
   ],
   "source": [
    "# These are the xpaths we determined from snooping \n",
    "next_button_xpath = \"//div[@class='row load-more']//a[contains(@href, 'startTime')]/@href\"\n",
    "headline_xpath = '//header/h1/a/text()'\n",
    "\n",
    "# We'll use sleep to add some time in between requests\n",
    "# so that we're not bombarding Gawker's server too hard. \n",
    "from time import sleep\n",
    "\n",
    "# Now we'll fill this list of gawker titles by starting\n",
    "# at the landing page and following \"More Stories\" links\n",
    "gawker_titles = []\n",
    "base_url = 'http://gawker.com/{}'\n",
    "next_page = \"http://gawker.com/\"\n",
    "while len(gawker_titles) < 2000 and next_page:\n",
    "    dom = html.parse(next_page)\n",
    "    headlines = dom.xpath(headline_xpath)\n",
    "    print \"Retrieved {} titles from url: {}\".format(len(headlines), next_page)\n",
    "    gawker_titles += headlines\n",
    "    next_pages = dom.xpath(next_button_xpath)\n",
    "    if next_pages: \n",
    "        next_page = base_url.format(next_pages[0]) \n",
    "    else:\n",
    "        print \"No next button found\"\n",
    "        next_page = None\n",
    "    sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holy smokes, we got 2017 Gawker headlines!\n"
     ]
    }
   ],
   "source": [
    "with open('gawker_titles.txt', 'wb') as out:\n",
    "    out.writelines(gawker_titles)\n",
    "# with open('gawker_titles.txt') as f:\n",
    "#     gawker_titles = f.readlines()\n",
    "    \n",
    "print \"Holy smokes, we got {} Gawker headlines!\".format(len(gawker_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Wall Street Journal](http://online.wsj.com/public/page/archive-2014-1-1.html)\n",
    "Now we'll do a similar thing with WSJ now. Here we notice that they have a section of the site where they have lists of articles for each day in the past year. There are links to the different archive dates all over the page, and we can see that the links all have the same structure, with different dates in the URL. Lets iterate over a bunch of dates. I grabbed the articles from the first day of each month this year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wsj_url = \"http://online.wsj.com/public/page/archive-2014-{}-1.html\"\n",
    "wsj_headline_xpath = \"//h2/a/text()\"\n",
    "wsj_headlines = []\n",
    "for i in range(1, 11): \n",
    "    dom = html.parse(wsj_url.format(i))\n",
    "    titles = dom.xpath(wsj_headline_xpath)\n",
    "    wsj_headlines += titles\n",
    "    print \"Retrieved {} WSJ headlines from url: {}\".format(len(titles), wsj_url.format(i))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('wsj_titles.txt', 'wb') as out:\n",
    "    out.writelines(wsj_headlines)\n",
    "# with open('wsj_titles.txt') as f:\n",
    "#     wsj_headlines = f.readlines()\n",
    "    \n",
    "print \"Jeez, Louise! We got {} WSJ headlines!\".format(len(wsj_headlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use pandas to build a data frame with two columns: \"gawker\", which contains a boolean value indicating whether the value in the \"title\" column came from Gawker's website. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gawk_records = [{'gawker': True, 'title': title} for title in gawker_titles]\n",
    "wsj_records = [{'gawker': False, 'title': title} for title in wsj_headlines]\n",
    "df = pd.DataFrame.from_records(gawk_records + wsj_records)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teach the Machine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic goal of machine learning is to learn a model or function that maps our inputs/observations to outputs/predictions. The model that we'll be building to make our predictions is known as Naive Bayes. We can compute by counting the probability that any given word shows up in a Gawker title, or P(Word | Gawker). We want the probability of a that a given body of text is a Gawker title, or P(Gawker | Words).\n",
    "\n",
    "$$P(Gawker | Words) = P(Word_1|Gawker)*P(Word_2|Gawker)*..*P(Word_n | Gawker)*P(Gawker)$$\n",
    "\n",
    "All of the probabilities on the right side of the equation are empirically determined from the text data. Luckily, instead of having to write all of the code to determine those probabilities ourselves, sklearn's CountVectorizer does all of the dirty work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import cross_validation, naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def make_xy(df, vectorizer=None):\n",
    "#     #Your code here    \n",
    "#     if vectorizer is None:\n",
    "#         vectorizer = CountVectorizer(min_df=5, max_df=.3, ngram_range=(1,2))\n",
    "#     X = vectorizer.fit_transform(df.title)\n",
    "#     X = X.tocsc()  # some versions of sklearn return COO format\n",
    "#     Y = df.gawker.values.astype(np.int)\n",
    "#     return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=5, max_df=.3, ngram_range=(1,2))\n",
    "# X, Y = make_xy(df)\n",
    "X = vectorizer.fit_transform(df.title)\n",
    "X = X.tocsc()  # some versions of sklearn return COO format\n",
    "Y = df.gawker.values.astype(np.int) # Need numbers instead of bools for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a data array, X, whose rows correspond to titles and whose columns correspond to words. So the value X[i, j] is the number of times word j shows up in article title i. Each row has a corresponding member in vector, Y. If the headline associated with X[i] came from Gawker, Y[i] == 1. Otherwise, Y[i] == 0. Now our data are in a format we want it to be in. Time to seperate our data into training and testing sets before building and evaluating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = naive_bayes.MultinomialNB(fit_prior=False, alpha=0.5)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the machine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see how good our model is by testing the accuracy of its predictions on articles it hasn't seen before. The accuracy metric reported below is simply the percentage of titles it classifies correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Accuracy: %0.2f%%\" % (100 * clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a stupid machine! We can take a closer look at how the model is making predictions. Let's look at which words are the Gawkeriest and which ones are the Wall Street Journaliest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "x = np.eye(X_test.shape[1])\n",
    "probs = clf.predict_log_proba(x)[:, 0]\n",
    "ind = np.argsort(probs)\n",
    "\n",
    "good_words = words[ind[:10]]\n",
    "bad_words = words[ind[-10:]]\n",
    "\n",
    "good_prob = probs[ind[:10]]\n",
    "bad_prob = probs[ind[-10:]]\n",
    "\n",
    "print \"Gawker words\\t P(gawker| word)\"\n",
    "for w, p in zip(good_words, good_prob):\n",
    "    print \"%20s\" % w, \"%0.2f\" % (1 - np.exp(p))\n",
    "    \n",
    "print \"WSJ words\\t P(gawker | word)\"\n",
    "for w, p in zip(bad_words, bad_prob):\n",
    "    print \"%20s\" % w, \"%0.2f\" % (1 - np.exp(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be complaining that our model should do better. But some titles can be tough. Would you have classified these mis-predicted article titles correctly? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob = clf.predict_proba(X)[:, 0]\n",
    "predict = clf.predict(X)\n",
    "\n",
    "bad_wsj = np.argsort(prob[Y == 0])[:5]\n",
    "bad_gawker = np.argsort(prob[Y == 1])[-5:]\n",
    "\n",
    "print \"Mis-predicted WSJ quotes\"\n",
    "print '---------------------------'\n",
    "for row in bad_wsj:\n",
    "    print df[Y == 0].title.irow(row)\n",
    "    print\n",
    "\n",
    "print\n",
    "print \"Mis-predicted Gawker quotes\"\n",
    "print '--------------------------'\n",
    "for row in bad_gawker:\n",
    "    print df[Y == 1].title.irow(row)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make your own headline and see if it belongs in Gawker or WSJ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = clf.predict_proba(vectorizer.transform([\"Your fucking title here\"]))\n",
    "print \"P(gawker) = {}\".format(probs[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
